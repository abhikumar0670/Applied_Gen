{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Module 3: Land Classification \u2014 CNN\u2013Transformer Integration Evaluation"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os, glob, numpy as np, matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\n\nDATASET_DIR = \"./images_dataSAT\"\nDIR_NON_AGRI = os.path.join(DATASET_DIR, \"class_0_non_agri\")\nDIR_AGRI = os.path.join(DATASET_DIR, \"class_1_agri\")\n\ndef _ensure_dataset():\n    os.makedirs(DIR_NON_AGRI, exist_ok=True)\n    os.makedirs(DIR_AGRI, exist_ok=True)\n    if len(os.listdir(DIR_NON_AGRI))>0 and len(os.listdir(DIR_AGRI))>0:\n        return\n    import numpy as np\n    from PIL import Image, ImageDraw\n    rng = np.random.default_rng(0)\n    for cls_dir, pattern in [(DIR_NON_AGRI, 'rect'), (DIR_AGRI, 'lines')]:\n        for i in range(12):\n            img = Image.new(\"RGB\",(64,64),(rng.integers(20,235),rng.integers(20,235),rng.integers(20,235)))\n            d = ImageDraw.Draw(img)\n            if pattern=='rect':\n                d.rectangle([10,10,54,54], outline=(255,255,255), width=2)\n            else:\n                for y in range(5,64,10):\n                    d.line([0,y,64,y], fill=(255,255,255), width=1)\n            img.save(os.path.join(cls_dir, f\"img_{{i:03d}}.png\"))\n\n# Copy dataset from /mnt/data if available\nif os.path.exists('/mnt/data/images_dataSAT'):\n    import shutil\n    if not os.path.exists(DATASET_DIR):\n        shutil.copytree('/mnt/data/images_dataSAT', DATASET_DIR)\n_ensure_dataset()\nprint(\"Dataset ready at\", os.path.abspath(DATASET_DIR))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport matplotlib.pyplot as plt\n\nDATASET_DIR = \"./images_dataSAT\"\nHYPERPARAMS = {\"image_size\": (64,64), \"batch_size\": 16, \"epochs\": 2}\nprint(\"Dataset:\", DATASET_DIR); print(\"Hyperparams:\", HYPERPARAMS)\ndef print_metrics(y_true, y_prob):\n    y_pred = (y_prob > 0.5).astype(int)\n    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n    print(\"Precision:\", precision_score(y_true, y_pred, zero_division=0))\n    print(\"Recall:\", recall_score(y_true, y_pred, zero_division=0))\n    print(\"F1:\", f1_score(y_true, y_pred, zero_division=0))\n    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Keras hybrid evaluation\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_keras_hybrid():\n    inp = layers.Input((64,64,3))\n    x = layers.Rescaling(1./255)(inp)\n    x = layers.Conv2D(16,3,activation='relu')(x); x = layers.MaxPooling2D()(x)\n    patches = tf.image.extract_patches(x, sizes=[1,8,8,1], strides=[1,8,8,1], rates=[1,1,1,1], padding='VALID')\n    patches = layers.Reshape((-1, patches.shape[-1]))(patches)\n    z = layers.MultiHeadAttention(num_heads=2, key_dim=32)(patches, patches)\n    z = layers.GlobalAveragePooling1D()(z)\n    out = layers.Dense(1, activation='sigmoid')(z)\n    return models.Model(inp, out)\n\nds_train = tf.keras.utils.image_dataset_from_directory(DATASET_DIR, validation_split=0.3, subset='training', seed=123, image_size=(64,64), batch_size=16)\nds_val = tf.keras.utils.image_dataset_from_directory(DATASET_DIR, validation_split=0.3, subset='validation', seed=123, image_size=(64,64), batch_size=16)\nmodel_k = build_keras_hybrid(); model_k.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_k.fit(ds_train, validation_data=ds_val, epochs=2, verbose=0)\ny_true, y_prob = [], []\nfor X,y in ds_val:\n    p = model_k.predict(X, verbose=0).ravel()\n    y_true.extend(y.numpy().ravel()); y_prob.extend(p)\nprint(\"Keras hybrid metrics:\"); print_metrics(np.array(y_true), np.array(y_prob))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# PyTorch hybrid evaluation\nimport torch\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntransform = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\nfull = datasets.ImageFolder(DATASET_DIR, transform=transform)\nn_val = int(0.3*len(full)); n_train = len(full)-n_val\ntrain_subset, val_subset = torch.utils.data.random_split(full, [n_train, n_val], generator=torch.Generator().manual_seed(123))\ntrain_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n\nclass CNNViTPT(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stem = nn.Sequential(nn.Conv2d(3,16,3,padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        self.proj = nn.Conv2d(16,32,kernel_size=8, stride=8)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=2, batch_first=True)\n        self.enc = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        self.fc = nn.Linear(32,1)\n    def forward(self,x):\n        x = self.stem(x); x = self.proj(x)\n        B,C,H,W = x.shape\n        x = x.flatten(2).transpose(1,2)\n        x = self.enc(x).mean(dim=1)\n        return self.fc(x).squeeze(1)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nm = CNNViTPT().to(device); opt = torch.optim.Adam(m.parameters(), 1e-3); crit = nn.BCEWithLogitsLoss()\nfor _ in range(2):\n    m.train()\n    for X,y in train_loader:\n        X, y = X.to(device), y.float().to(device)\n        opt.zero_grad(); out = m(X); loss = crit(out,y); loss.backward(); opt.step()\n\nm.eval(); probs=[]; trues=[]\nwith torch.no_grad():\n    for X,y in val_loader:\n        X = X.to(device); out = m(X)\n        probs.extend(torch.sigmoid(out).cpu().numpy().ravel()); trues.extend(y.numpy().ravel())\nprint(\"PyTorch hybrid metrics:\"); print_metrics(np.array(trues), np.array(probs))"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Module 3: Vision Transformers in PyTorch"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os, glob, numpy as np, matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\n\nDATASET_DIR = \"./images_dataSAT\"\nDIR_NON_AGRI = os.path.join(DATASET_DIR, \"class_0_non_agri\")\nDIR_AGRI = os.path.join(DATASET_DIR, \"class_1_agri\")\n\ndef _ensure_dataset():\n    os.makedirs(DIR_NON_AGRI, exist_ok=True)\n    os.makedirs(DIR_AGRI, exist_ok=True)\n    if len(os.listdir(DIR_NON_AGRI))>0 and len(os.listdir(DIR_AGRI))>0:\n        return\n    import numpy as np\n    from PIL import Image, ImageDraw\n    rng = np.random.default_rng(0)\n    for cls_dir, pattern in [(DIR_NON_AGRI, 'rect'), (DIR_AGRI, 'lines')]:\n        for i in range(12):\n            img = Image.new(\"RGB\",(64,64),(rng.integers(20,235),rng.integers(20,235),rng.integers(20,235)))\n            d = ImageDraw.Draw(img)\n            if pattern=='rect':\n                d.rectangle([10,10,54,54], outline=(255,255,255), width=2)\n            else:\n                for y in range(5,64,10):\n                    d.line([0,y,64,y], fill=(255,255,255), width=1)\n            img.save(os.path.join(cls_dir, f\"img_{{i:03d}}.png\"))\n\n# Copy dataset from /mnt/data if available\nif os.path.exists('/mnt/data/images_dataSAT'):\n    import shutil\n    if not os.path.exists(DATASET_DIR):\n        shutil.copytree('/mnt/data/images_dataSAT', DATASET_DIR)\n_ensure_dataset()\nprint(\"Dataset ready at\", os.path.abspath(DATASET_DIR))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import torch, time, numpy as np, matplotlib.pyplot as plt\nfrom torch import nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntrain_tf = transforms.Compose([transforms.Resize((64,64)), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\nval_tf = transforms.Compose([transforms.Resize((64,64)), transforms.ToTensor()])\nfull = datasets.ImageFolder(DATASET_DIR)\nn_val = int(0.3*len(full)); n_train = len(full)-n_val\ntrain_subset, val_subset = torch.utils.data.random_split(full, [n_train, n_val], generator=torch.Generator().manual_seed(123))\ntrain_subset.dataset.transform = train_tf; val_subset.dataset.transform = val_tf\ntrain_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n\nclass PatchEmbed(nn.Module):\n    def __init__(self, emb=64):\n        super().__init__()\n        self.proj = nn.Conv2d(3, emb, kernel_size=8, stride=8)\n    def forward(self, x):\n        x = self.proj(x)              # (B, emb, H', W')\n        return x.flatten(2).transpose(1,2)  # (B, N, emb)\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, emb=64, heads=2, depth=2, mlp=128):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                nn.MultiheadAttention(emb, heads, batch_first=True),\n                nn.Sequential(nn.LayerNorm(emb), nn.Linear(emb, mlp), nn.GELU(), nn.Linear(mlp, emb))\n            ]))\n        self.norm = nn.LayerNorm(emb)\n    def forward(self, x):\n        for mha, mlp in self.layers:\n            attn_out,_ = mha(x, x, x)\n            x = self.norm(x + attn_out)\n            x = self.norm(x + mlp(x))\n        return x\n\nclass CNNViT(nn.Module):\n    def __init__(self, emb=64, depth=2):\n        super().__init__()\n        self.patch = PatchEmbed(emb=emb)\n        self.enc = TransformerEncoder(emb=emb, heads=2, depth=depth, mlp=emb*2)\n        self.fc = nn.Linear(emb, 1)\n    def forward(self, x):\n        x = self.patch(x)\n        x = self.enc(x).mean(dim=1)\n        return self.fc(x).squeeze(1)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = CNNViT(emb=64, depth=2).to(device)\nmodel_test = CNNViT(emb=32, depth=1).to(device)\n\ncrit = nn.BCEWithLogitsLoss()\nopt = torch.optim.Adam(model.parameters(), 1e-3)\nopt2 = torch.optim.Adam(model_test.parameters(), 1e-3)\n\ndef train_one(m, opt):\n    m.train(); t0=time.time(); losses=[]\n    for X,y in train_loader:\n        X, y = X.to(device), y.float().to(device)\n        opt.zero_grad(); out = m(X); loss = crit(out,y); loss.backward(); opt.step()\n        losses.append(loss.item())\n    return np.mean(losses), time.time()-t0\n\ndef validate(m):\n    m.eval(); losses=[]\n    with torch.no_grad():\n        for X,y in val_loader:\n            X, y = X.to(device), y.float().to(device)\n            out = m(X); loss = crit(out,y); losses.append(loss.item())\n    return np.mean(losses)\n\ntrain_losses_m, val_losses_m, times_m = [], [], []\ntrain_losses_t, val_losses_t, times_t = [], [], []\nepochs=3\nfor e in range(epochs):\n    tl, tm = train_one(model, opt); vl = validate(model)\n    train_losses_m.append(tl); val_losses_m.append(vl); times_m.append(tm)\n    tl2, tm2 = train_one(model_test, opt2); vl2 = validate(model_test)\n    train_losses_t.append(tl2); val_losses_t.append(vl2); times_t.append(tm2)\n    print(f\"Epoch {e+1}: model val_loss={vl:.3f} ({tm:.2f}s) | model_test val_loss={vl2:.3f} ({tm2:.2f}s)\")\n\nplt.figure(); plt.plot(val_losses_m, label='model val_loss'); plt.plot(val_losses_t, label='model_test val_loss'); plt.legend(); plt.title('Validation Loss'); plt.show()\nplt.figure(); plt.plot(times_m, label='model sec/epoch'); plt.plot(times_t, label='model_test sec/epoch'); plt.legend(); plt.title('Epoch Training Time'); plt.show()"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}
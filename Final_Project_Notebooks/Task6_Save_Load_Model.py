{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Task 6: Save and load trained model (state_dict)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import os, numpy as np, torch\nfrom torch import nn\nif not os.path.exists(\"X_train.npy\"): raise FileNotFoundError(\"Run Task1 first.\")\nX_train = np.load(\"X_train.npy\"); y_train = np.load(\"y_train.npy\")\nX_test = np.load(\"X_test.npy\"); y_test = np.load(\"y_test.npy\")\nX_train_t = torch.tensor(X_train, dtype=torch.float32); y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1,1)\nX_test_t = torch.tensor(X_test, dtype=torch.float32); y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1,1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "class LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim):\n        super().__init__(); self.linear = nn.Linear(input_dim,1)\n    def forward(self,x): return torch.sigmoid(self.linear(x))\n\nmodel = LogisticRegressionModel(X_train_t.shape[1])\nopt = torch.optim.SGD(model.parameters(), lr=0.01)\n# quick training\nloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\nfor epoch in range(10):\n    for xb, yb in loader:\n        opt.zero_grad(); out = model(xb); loss = nn.BCELoss()(out, yb); loss.backward(); opt.step()\n\n# Save state_dict\ntorch.save(model.state_dict(), \"logreg_state.pth\")\nprint(\"Saved state_dict to logreg_state.pth\")\n\n# Load into new model\nloaded = LogisticRegressionModel(X_train_t.shape[1])\nloaded.load_state_dict(torch.load(\"logreg_state.pth\"))\nloaded.eval()\nwith torch.no_grad():\n    acc_loaded = (loaded(X_test_t)>=0.5).float().eq(y_test_t).float().mean().item()\nprint(\"Loaded model test acc:\", acc_loaded)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5}